#!/usr/bin/env python
from __future__ import print_function
import jinja2
import json
import jdl2makeflow_helpers
from jdl2makeflow_helpers import classad
import re, os, errno, sys
from shutil import copy2, rmtree
from argparse import ArgumentParser, REMAINDER
import subprocess

lib_path = os.path.abspath(os.path.dirname(jdl2makeflow_helpers.__file__))

ap = ArgumentParser()
ap.add_argument("--dryrun", "-n", dest="dry_run", default=False, action="store_true",
                help="Generate Makeflow script that does not actually run any job")
ap.add_argument("--workdir", "-w", dest="work_dir", default="work",
                help="Makeflow work directory (defaults to \"work\")")
ap.add_argument("--parse-jdl", "-p", dest="parse_jdl", default=False, action="store_true",
                help="Only parse the JDL, show the result and exit")
ap.add_argument("--summary", "-s", dest="summary", default=False, action="store_true",
                help="Only show summary without creating any file and exit")
ap.add_argument("--start-at", "-t", dest="start_at", default=None,
                choices=["sim", "merge", "qaplots", "cpass0", "cpass0merge", "tpcspc", "cpass1", "cpass1merge", "cpass1qa", "cpass1qaplots" ],
                help="Start at the defined stage, assuming the previous stages were successful")
ap.add_argument("--force", dest="force", default=False, action="store_true",
                help="Force removal of working directory (you will lose files!)")
ap.add_argument("--graph", dest="graph", default=False, action="store_true",
                help="Create dependency graph as Makeflow.pdf in the workdir")
ap.add_argument("--run", "-r", dest="run", default=False, action="store_true",
                help="Automatically run workflow after generating the manifest")
ap.add_argument("jdl",
                help="AliEn JDL steering the jobs")
ap.add_argument("makeflow_opts", nargs=REMAINDER,
                help="Options for the makeflow command")
args = ap.parse_args()

j2env = jinja2.Environment()
j2env.filters["basename"] = lambda x: os.path.basename(x)

def preprocess_var(s, sh_mode):
  if isinstance(s, list):
    s = ",".join(s)
  if not re.search("#alien_counter(_[^#]+)?#|#alienfilename(/([^/#]+)/([^/#]*)/)?#", s):
    return s
  if sh_mode:
    # Shell mode: rely on script's alienfmt() shell function
    s = "$(alienfmt '%s')" % s
  else:
    # Jinja mode: use Jinja's builtin string formatting
    s = s.replace("#alien_counter#", "#alien_counter_i#")
    s = re.sub("#alien_counter_([^#]+)#", "{{'%\\1'|format(alien_counter)}}", s)
    s = s.replace("#alienfilename#", "{{input_files[alien_counter]|basename()}}")
    s = re.sub("#alienfilename/([^/#]+)/([^/#]*)/#", "{{input_files[alien_counter]|basename()|replace('\\1','\\2')}}", s)
  return s

def gen_runjob(output_file, jdl, dry_run):
  runjob = j2env.from_string("""#!/bin/bash -e
type zip unzip zipinfo
ulimit -c unlimited  # enable core dumps
{% for v in [ "LANG", "LANGUAGE", "LC_ALL", "LC_COLLATE", "LC_CTYPE", "LC_MESSAGES", \
              "LC_MONETARY", "LC_NUMERIC", "LC_TIME", "LC_ALL" ] -%}
export {{v}}=C
{% endfor -%}
JOBID=$1
JOBDIR=$(basename "$0")
JOBDIR=job-${JOBDIR%.*}-$(printf %04d $JOBID)
DONEFILE="$PWD/$2"
INPUTFILE="$3"

function alienfmt() {
  local ALIENFMT_STR="$1"
  local INPUTFILE_BASE=${INPUTFILE##*/}

  RE='^(.*)#alien_counter(_([^#]+))?#(.*)$'
  while [[ $ALIENFMT_STR =~ $RE ]]; do
    REPLACE=${BASH_REMATCH[3]}
    [[ $REPLACE ]] || REPLACE=i
    REPLACE=$(printf %${REPLACE} $JOBID)
    ALIENFMT_STR="${BASH_REMATCH[1]}${REPLACE}${BASH_REMATCH[4]}"
  done

  RE='^(.*)#alienfilename(/([^#/]+/[^#/]*)/)?#(.*)$'
  while [[ $ALIENFMT_STR =~ $RE ]]; do
    REPLACE=${BASH_REMATCH[3]}
    [[ $REPLACE ]] && REPLACE='${INPUTFILE_BASE//'$REPLACE'}' || REPLACE='$INPUTFILE_BASE'
    ALIENFMT_STR="${BASH_REMATCH[1]}${REPLACE}${BASH_REMATCH[4]}"
  done

  ALIENFMT_STR=$(eval printf -- \\""$ALIENFMT_STR"\\")
  echo "$ALIENFMT_STR"
}

rm -rf job-$JOBDIR
mkdir job-$JOBDIR
cd job-$JOBDIR
INPUT_LIST=({% for x in input_list -%}
            "{{x}}"
            {% endfor %})
OUTPUT_LIST=({% for x in output_list -%}
             "{{x}}"
             {% endfor %})
for INP in "${INPUT_LIST[@]}"; do
  PROTO=${INP%%://*}
  UNPACK=
  [[ $PROTO != $INP ]] || { PROTO=local; [[ $INP == /* ]] || INP="../$INP"; }
  [[ $PROTO == local || $PROTO == root ]] || { echo "Input protocol $PROTO not supported"; exit 1; }
  [[ $INP == *,unpack ]] && { UNPACK=1; INP=${INP//,unpack}; }
  for ((I=1; I<=5; I++)); do
    ERR=0
    echo "Transferring $INP to the input box (attempt $I/5)"
    case $PROTO in
      local) cp -v "$INP" . && break || ERR=$? ;;
      root)  xrdcp -f "$INP" . && break || ERR=$? ;;
    esac
  done
  [[ $ERR == 0 ]] || { echo "Error copying files into the input box"; exit 1; }
  if [[ $UNPACK ]]; then
    INP=${INP##*/}
    echo "Unpacking then removing $INP"
    case "$INP" in
      *.tar.bz2) UNPACK_CMD="tar xjvvf" ;;
      *.tar.gz)  UNPACK_CMD="tar xzvvf" ;;
      *.zip)     UNPACK_CMD="unzip"     ;;
      *) echo "Unsupported archive format: $INP"; exit 1; ;;
    esac
    $UNPACK_CMD "$INP" || ERR=$?
    rm -f "$INP"
  fi
  [[ $ERR == 0 ]] || { echo "Error unpacking files into the input box"; exit 1; }
done
echo "List of files in the working directory (recursive)"
find . -ls
echo "List of files ends here"
{% if env_cmd -%}
# Set custom environment (handling potential concurrency issues on our side)
LOCKDIR=/tmp/makeflow_env_lock
MAXWAIT=1000
sleep 0.1 &> /dev/null || MAXWAIT=100
for ((I=0; I<$MAXWAIT; I++)); do
  mkdir $LOCKDIR &> /dev/null && break || true
  sleep 0.1 || sleep 1
done
set +e
{{env_cmd}}
ENV_RV=$?
set -e
rmdir $LOCKDIR || true
[[ $ENV_RV == 0 ]] || exit 3
{% else -%}
eval $(/cvmfs/alice.cern.ch/bin/alienv printenv {{packages|join(",")}})
{% endif -%}
{{""}}
# Job environment
{% for x in environment -%}
export {{x}}="{{environment[x]|replace('"', '\\\\"')}}"
{% endfor -%}
{{""}}
echo "Output will be in $ALIEN_JDL_OUTPUTDIR"

# Execute command and ignore errors
ARGS="{{args}}"
PROG="{{executable}}"
{% if dry_run -%}
echo "Doing nothing: dry run" >> stdout.log
MAINERR=0
{% else -%}
type "$PROG" &> /dev/null || PROG="{{executable|basename()}}"
type "$PROG" &> /dev/null || PROG="$(find "$ALIDPG_ROOT" -name "{{executable|basename()}}" -print -quit || true)"
[[ "$PROG" ]] || PROG="./{{executable|basename()}}"
[[ -x "$PROG" ]] || PROG="bash $PROG"
MAINERR=0
{ env; echo + $PROG $ARGS; } >> stdout.log
$PROG $ARGS  > >(tee -a stdout.log{% if no_live_out %} &> /dev/null{% endif %}) \\
            2> >(tee -a stderr.log{% if no_live_out %} &> /dev/null{% endif %}) || MAINERR=$?
{% endif -%}
[[ $MAINERR != 0 ]] && echo "Exited with errors ($MAINERR)" || echo "Exited with no errors";
{ echo + $PROG $ARGS "exited with $MAINERR"; } >> stdout.log

# Produce validation report
touch validation_report.txt
grep -H -E 'std::bad_alloc|Segmentation violation|Segmentation fault|Bus error|floating point exception|Killed|busy flag cleared|Cannot Build the PAR Archive|\\*\\*\\* glibc detected \\*\\*\\*|E-AliCDBGrid::PutEntry:|F-AliCDBGrid::|E-TAlienFile::ReadBuffer: The remote \\(removed\\) file is not open' *.log *.nonesiste > validation_report.txt 2> /dev/null || true
for CORE in core*; do
  [[ -e $CORE ]] || continue
  echo "$CORE: core dumped"
done > validation_report.txt 2> /dev/null

# Compress output according to the output list
mkdir to_transfer
shopt -s extglob
for OUT in "${OUTPUT_LIST[@]}"; do
  ZIP=${OUT%%:*}
  FILES=${OUT#*:}
  [[ $ZIP == $OUT ]] && { echo "Not archiving $OUT"; mv -v $OUT to_transfer/ || true; continue; }
  [[ $FILES =~ \.root(,|$) ]] && ZIP_COMP="-0" || ZIP_COMP="-9"
  FILES=${FILES//,/ }
  echo $ZIP will contain $FILES
  ZIPERR=0
  zip $ZIP_COMP tmparchive.zip $FILES || ZIPERR=$?  # exitcode 12 is fine ==> "nothing to do"
  [[ $ZIPERR == 12 ]] && { echo "Zip $ZIP would be empty: not creating"; continue; } \\
                      || [[ $ZIPERR == 0 ]]
  rm -f $FILES  # same files cannot be in more than one archive
  mv tmparchive.zip to_transfer/$ZIP
done

# Copy files to destination (filesystem or xrootd)
PROTO=${ALIEN_JDL_OUTPUTDIR%%://*}
[[ $PROTO != $ALIEN_JDL_OUTPUTDIR ]] || { PROTO=local; mkdir -p $ALIEN_JDL_OUTPUTDIR; }
[[ $PROTO == local || $PROTO == root ]] || { echo "Output protocol $PROTO not supported;" exit 1; }
pushd to_transfer
  while read FILE; do
    for ((I=1; I<=5; I++)); do
      ERR=0
      echo "Transferring $FILE to $ALIEN_JDL_OUTPUTDIR (attempt $I/5)"
      case $PROTO in
        local) mkdir -p $(dirname "$ALIEN_JDL_OUTPUTDIR/$FILE"); cp -v $FILE $ALIEN_JDL_OUTPUTDIR/$FILE && break || ERR=$? ;;
        root)  xrdcp -f $FILE $ALIEN_JDL_OUTPUTDIR/$FILE && break || ERR=$? ;;
      esac
    done
  done < <(find . -type f | sed -e 's|^\./||')
popd

# Cleanup all
rm -rf *

# Signal success
echo Workflow execution completed: $PROG $ARGS exited with $MAINERR
touch $DONEFILE
""")
  with open(output_file, "w") as mf:
    mf.write(runjob.render(output_list = jdl["Output"],
                           input_list  = jdl["InputFile"],
                           packages    = jdl["Packages"],
                           env_cmd     = jdl.get("EnvironmentCommand", None),
                           no_live_out = jdl["NoLiveOutput"],
                           environment = jdl["Environment"],
                           dry_run     = dry_run,
                           executable  = jdl["Executable"],
                           args        = jdl["SplitArguments"]))
  os.chmod(output_file, int("755", 8))

def get_alien_xml(pattern, joba, jobb, input_files):
  axml = j2env.from_string(
"""<alien>
  <collection name="alien_collection.xml">
    {%- for alien_counter in range(joba,jobb+1) %}
    <event name="{{alien_counter}}">
      <file turl="***TURL***" type="f"/>
    </event>{% endfor %}
  </collection>
</alien>
""".replace("***TURL***", preprocess_var(pattern, sh_mode=False)))
  return axml.render(joba=joba, jobb=jobb, input_files=input_files)

def get_preprocessed_jdl(jdl_fn, override={}, append={}, delete=[]):
  jdl = classad.parse(open(jdl_fn).read(), ignore_errors=True)

  # Process overrides
  for k in override:
    jdl[k] = override[k]

  # Consider Arguments and SplitArguments equivalent
  if "Arguments" in jdl:
    jdl["SplitArguments"] = jdl["Arguments"]

  # Command-line arguments, supporting AliEn variables
  jdl["SplitArguments"] = preprocess_var(jdl["SplitArguments"], sh_mode=True)

  # Packages (filter out jemalloc)
  jdl["Packages"] = [ x for x in jdl["Packages"] if not "jemalloc" in x ]

  # Read input files list (optional)
  try:
    jdl["InputFilesFromList"] = [ x.strip() for x in
                                  open(os.path.basename(jdl.get("InputDataCollection", ""))).readlines()
                                  if not re.search("(^\s*#|^\s*$)", x) ]
  except Exception as e:
    jdl["InputFilesFromList"] = []

  # Job range ("Split" parameter)
  m = re.search("^([^:]+)(:([0-9]+)-([0-9]+))?$", jdl["Split"])
  if m.group(3) and m.group(4):
    jdl["JobRange"] = [ int(m.group(3)), int(m.group(4)) ]  # production
  else:
    jdl["JobRange"] = [ 0, max(0, len(jdl["InputFilesFromList"])-1) ]  # file

  # Remove @disk spec from output list
  jdl["Output"] = [ o.split("@", 1)[0] for o in jdl["Output"] ]

  # Input list: base path only (assume files are in the cwd or a "system" one), exclude OCDB
  jdl["InputFile"] = [ os.path.basename(x) if x.startswith("LF:") else x
                       for x in jdl.get("InputFile", []) ]
  jdl["InputFile"] = [ x for x in jdl["InputFile"] if not x.startswith("OCDB") ]

  # Process appends
  for k in append:
    if k in jdl:
      jdl[k] += append[k]

  if not "NextStages" in jdl:
    if jdl.get("Executable", "").endswith("aliroot_dpgsim.sh"):
      jdl["NextStages"] = [ "FinalQA", "MergeFilteredTrees" ]
    elif "/aliroot_dpg" in jdl.get("Executable", ""):
      jdl["NextStages"] = [ "MergeMakeOCDB_CPass0", "MergeFilteredTrees",
                            "TPCSPVDTime",
                            "CPass1",
                            "MergeMakeOCDB_CPass1",
                            "FinalQA" ]

  # Job environment
  environment = {}
  for v in jdl.get("JDLVariables", []):
    environment["ALIEN_JDL_"+v.upper()] = preprocess_var(jdl.get(v, ""), sh_mode=True)  # with ALIEN_JDL_
  for v in jdl.get("ExtraVariables", []):
    environment[v] = preprocess_var(jdl.get(v, ""), sh_mode=True)  # exported as-is
  jdl["Environment"] = environment

  # Output requested not in zip files
  if jdl.get("DontArchive", None) == "1":
    jdl["DontArchive"] = True
    new_output = []
    for o in jdl["Output"]:
      if ":" in o:
        new_output += o.split(":", 1)[1].split(",")
      else:
        new_output.append(o)
    jdl["Output"] = new_output
  else:
    jdl["DontArchive"] = False

  # Save everything but no input files
  if jdl.get("SaveAll", None) == "1":
    jdl["SaveAll"] = True
    jdl["Output"] = [ "!(%s)" % "|".join(jdl["InputFile"]) ]
  else:
    jdl["SaveAll"] = False

  # No live output (convert to boolean)
  jdl["NoLiveOutput"] = jdl.get("NoLiveOutput", None) == "1"

  # Remove unneeded variables (cleanup)
  all_vars = jdl.keys()
  whitelist = [ "SplitArguments", "Executable", "Packages", "JobRange", "JobRange", "Output",
                "InputFile", "Environment", "NextStages", "OutputDir", "EnvironmentCommand",
                "SaveAll", "DontArchive", "NoLiveOutput", "InputFilesFromList" ] + \
              list(override.keys()) + list(append.keys())
  for k in list(all_vars):
    if k in delete or k not in whitelist:
      del jdl[k]

  return jdl

# First tier of jobs
all_inputs = {}
jdl = get_preprocessed_jdl(args.jdl)
all_inputs["Jobs"] = jdl["InputFile"]

# Define start_at based on NextStages
if not args.start_at:
  args.start_at = "cpass0" if "MergeMakeOCDB_CPass0" in jdl.get("NextStages", []) else "sim"

# Reco CPass0: merge and make OCDB
jdl_reco_mergemakeocdbcpass0 = {}
if "MergeMakeOCDB_CPass0" in jdl.get("NextStages", []):
  runNumber = int(os.path.basename(jdl["InputFilesFromList"][0])[2:11])
  jdl_reco_mergemakeocdbcpass0 = \
    get_preprocessed_jdl(args.jdl,
      override={ "Output": [ "log_archive.zip:*.log@disk=2",
                             "ocdb_archive.zip:CalibObjects.root,meanITSVertex.root,fitITSVertex.root,cpassStat.root@disk=4",
                             "OCDB.tar.bz2",
                             "validation_report.txt",
                             "core*" ],
                 "OutputDir": os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)), "OCDB"),
                 "Executable": "aliroot_dpgMergeMakeOCDB_CPass0.sh",
                 "Arguments": "calibobjects.xml %d local://./OCDB fileAccessMethod=copyXMLcollection makeOCDB=1 calibObjectsFileName=CalibObjects.root" % runNumber,
                 "CreateOCDBArchive": "1"
               },
      append={ "InputFile"   : [ "calibobjects.xml" ],
               "JDLVariables": [ "CreateOCDBArchive" ] },
      delete=[ "NextStages", "JobRange", "InputFilesFromList" ])
  all_inputs["MergeMakeOCDB_CPass0"] = jdl_reco_mergemakeocdbcpass0["InputFile"]
  xml_reco = get_alien_xml(os.path.join(jdl["OutputDir"], "CalibObjects.root"),
                           jdl["JobRange"][0], jdl["JobRange"][1], jdl["InputFilesFromList"])

# Reco CPass0: TPCSPVDTime
jdl_reco_tpcspvdtime = {}
if "TPCSPVDTime" in jdl.get("NextStages", []):
  runNumberStr = os.path.basename(jdl["InputFilesFromList"][0])[2:11]
  jdl_reco_tpcspvdtime = \
    get_preprocessed_jdl(args.jdl,
      override={ "Output": [ "log_archive.zip:*.log,*.list@disk=2",
                             "root_archive.zip:*.root@disk=2",
                             "validation_report.txt",
                             "core*" ],
                 "OutputDir": os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)), "ResidualMerge", "TPCSPCalibration"),
                 "Executable": "aliroot_procVDTime.sh",
                 "Arguments": "tpcSPCalibration.xml %s" % runNumberStr,
                 "targetOCDBDir": "local://./OCDB",  # output for OCDB files, it's "same" (raw://) on the Grid (OCDB is prepopulated from tarball)
                 "useTOFBC": "false"
               },
      append={ "InputFile": [ "tpcSPCalibration.xml",
                              os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)), "OCDB", "OCDB.tar.bz2,unpack") ],
               "JDLVariables": [ "targetOCDBDir", "useTOFBC" ] },
      delete=[ "NextStages", "JobRange", "InputFilesFromList" ])
  all_inputs["TPCSPVDTime"] = jdl_reco_tpcspvdtime["InputFile"]
  xml_tpcspcalibration = get_alien_xml(
    os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)), "ResidualMerge", "001", "ResidualTrees.root"),
    0, 0, [])

# Reco CPass1
jdl_reco_cpass1 = {}
if "CPass1" in jdl.get("NextStages", []):
  jdl_reco_cpass1 = \
    get_preprocessed_jdl(args.jdl,
      override={ "Output": [ "log_archive.zip:qa*.log,calib.log,filtering.log,mergeQA_outer.log,mergeQA_barrel.log,rec.log,stderr.log,stdout.log@disk=1",
                             "root_archive.zip:AliESDs_Barrel.root,CalibObjects.root,*.ESD.tag.root,TOFcalibTree.root,T0AnalysisTree.root@disk=2",
                             "QA_archive.zip:QAresults_barrel.root,QAresults_outer.root,*.stat.qa*@disk=2",
                             "EventStat_temp*.root@disk=2",
                             "ResidualTrees.root@disk=2",
                             "FilterEvents_Trees.root@disk=2",
                             "validation_report.txt",
                             "core*" ],
                 "OutputDir": jdl["OutputDir"].replace("/cpass0_pass1/", "/cpass1_pass1/"),
                 "Executable": "aliroot_dpgCPass1.sh",
                 "LPMPass": "1",
                 "LPMPassName": "cpass1_pass1",
                 "LPMRAWPassID": "1",
                 "LPMCPassMode": "1"
               },
      append={ "InputFile"   : [ os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)), "OCDB", "OCDB.tar.bz2,unpack") ] },
      delete=[ "NextStages" ])
  all_inputs["CPass1"] = jdl_reco_cpass1["InputFile"]

# Reco CPass1: merge and make OCDB
jdl_reco_mergemakeocdbcpass1 = {}
if "MergeMakeOCDB_CPass1" in jdl.get("NextStages", []):
  runNumber = int(os.path.basename(jdl["InputFilesFromList"][0])[2:11])
  jdl_reco_mergemakeocdbcpass1 = \
    get_preprocessed_jdl(args.jdl,
      override={ "Output": [ "log_archive.zip:*.log@disk=2",
                             "ocdb_archive.zip:CalibObjects.root,meanITSVertex.root,fitITSVertex.root,cpassStat.root@disk=4",
                             "OCDB.tar.bz2",
                             "validation_report.txt",
                             "core*" ],
                 "OutputDir": os.path.join(os.path.dirname(preprocess_var(jdl_reco_cpass1["OutputDir"], sh_mode=False)), "OCDB"),
                 "Executable": "aliroot_dpgMergeMakeOCDB_CPass1.sh",
                 "Arguments": "calibobjects_cpass1.xml %d local://./OCDB fileAccessMethod=copyXMLcollection makeOCDB=1 calibObjectsFileName=CalibObjects.root" % runNumber,
                 "CreateOCDBArchive": "1"
               },
      append={ "InputFile"   : [ "calibobjects_cpass1.xml",
                                 os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)), "OCDB", "OCDB.tar.bz2,unpack") ],
               "JDLVariables": [ "CreateOCDBArchive" ] },
      delete=[ "NextStages", "JobRange", "InputFilesFromList" ])
  all_inputs["MergeMakeOCDB_CPass1"] = jdl_reco_mergemakeocdbcpass1["InputFile"]
  xml_calib_cpass1 = get_alien_xml(os.path.join(jdl_reco_cpass1["OutputDir"], "CalibObjects.root"),
                                   jdl_reco_cpass1["JobRange"][0], jdl_reco_cpass1["JobRange"][1],
                                   jdl_reco_cpass1["InputFilesFromList"])

# Second tier of jobs: merging of filtered trees
jdl_mft = {}
if "MergeFilteredTrees" in jdl.get("NextStages", []):
  filtered_trees_fn = "ResidualTrees.root" if "MergeMakeOCDB_CPass0" in jdl.get("NextStages", []) else "FilterEvents_Trees.root"
  jdl_mft = get_preprocessed_jdl(args.jdl,
              override={ "Output": [ "spcm_archive.zip:pyxsec*.root,AODQA.root,AliAOD*.root,FilterEvents_Trees*.root,*.stat*,EventStat_temp*.root,Residual*.root,TOFcalibTree.root,std*,fileinfo*.log@disk=2",
                                     "validation_report.txt",
                                     "core*" ],
                         "OutputDir": os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)),
                                                   "ResidualMerge" if "MergeMakeOCDB_CPass0" else "SpacePointCalibrationMerge",
                                                   "001"),
                         "Executable": "merge.sh",
                         "InputXML": "residualtrees.xml",
                         "LPMCollectionEntity": filtered_trees_fn,
                         "NoGridConnect": "1",
                         "SplitArguments": "" },
              append={ "InputFile": [ "residualtrees.xml" ],
                       "JDLVariables": [ "LPMCollectionEntity", "InputXML", "NoGridConnect" ] },
              delete=[ "NextStages", "JobRange", "InputFilesFromList" ])
  xml_mft = get_alien_xml(os.path.join(jdl["OutputDir"], filtered_trees_fn),
                          jdl["JobRange"][0], jdl["JobRange"][1], jdl["InputFilesFromList"])
  all_inputs["MergeFilteredTrees"] = jdl_mft["InputFile"]

# Second tier of jobs: FinalQA
jdl_fqa = {}
if "FinalQA" in jdl.get("NextStages", []):
  isMC = not "CPass1" in jdl.get("NextStages", [])
  jdl_base = jdl if isMC else jdl_reco_cpass1  # do it for CPass1 on reconstruction
  inputfile_fqa = "finalqa.txt" if jdl["DontArchive"] or jdl["SaveAll"] else "finalqa.xml"
  jdl_fqa = get_preprocessed_jdl(args.jdl,
              override={ "Output": [ "QA_merge_log_archive.zip:std*,fileinfo*.log@disk=1",
                                     "QA_merge_archive.zip:*QAresults*.root,EventStat_temp*.root,trending*.root,event_stat*.root,*.stat*@disk=3",
                                     "validation_report.txt",
                                     "core*" ],
                         "OutputDir": os.path.dirname(preprocess_var(jdl_base["OutputDir"], sh_mode=False)),
                         "Executable": "aliprod_train_merge.sh" if isMC else "train_merge.sh",
                         "SplitArguments": "%s 5" % inputfile_fqa},  # 5 means "final stage"
              append={ "InputFile": [ inputfile_fqa ] + ([ "aliprod_train_merge.sh" ] if isMC else []) },
              delete=[ "NextStages", "JobRange" ])
  if inputfile_fqa.endswith(".xml"):
    inputlist_fqa = get_alien_xml(os.path.join(jdl_base["OutputDir"], "QA_archive.zip"),
                                  jdl_base["JobRange"][0], jdl_base["JobRange"][1],
                                  jdl_base["InputFilesFromList"])
  else:
    pattern = preprocess_var(jdl_base["OutputDir"], sh_mode=False)
    inputlist_fqa = "\n".join([ j2env.from_string(pattern).render(alien_counter=ji, input_files=jdl_base["InputFilesFromList"])
                                for ji in range(jdl_base["JobRange"][0], jdl_base["JobRange"][1]+1) ]) + "\n"
  all_inputs["FinalQA"] = jdl_fqa["InputFile"]
  qap_inputs = [ "QAresults.root", "QAresults_merged.root", "QAresults_barrel.root",
                 "QAresults_outer.root", "FilterEvents_Trees.root", "event_stat.root",
                 "event_stat_barrel.root", "event_stat_outer.root" ] \
                if jdl["DontArchive"] or jdl["SaveAll"] else \
               [ "QA_merge_archive.zip" ]
  jdl_qap = get_preprocessed_jdl(args.jdl,
              override={ "Output": [ "qa_plots/*", "std*", "core*", "validation_report.txt" ],
                         "OutputDir": os.path.join(os.path.dirname(preprocess_var(jdl_base["OutputDir"], sh_mode=False)), "QAplots_passMC" if isMC else "QAPlots_CPass1"),
                         "Executable": "qa_plots.sh",
                         "SplitArguments": " ".join([ os.path.join(preprocess_var(os.path.dirname(jdl_base["OutputDir"]), sh_mode=False), x) for x in qap_inputs ]) },
              append={ "InputFile": [ "qa_plots.sh" ] },
              delete=[ "NextStages", "JobRange", "InputFilesFromList" ])
  all_inputs["QAPlots"] = jdl_qap["InputFile"]

if args.parse_jdl:
  print("# First tier of jobs")
  print(json.dumps(jdl, indent=2))
  if jdl_reco_mergemakeocdbcpass0:
    print("\n# Reco CPass0: merge and make OCDB")
    print(json.dumps(jdl_reco_mergemakeocdbcpass0, indent=2))
  if jdl_mft:
    print("\n# Merging of filtered trees")
    print(json.dumps(jdl_mft, indent=2))
  if jdl_reco_tpcspvdtime:
    print("\n# Reco CPass0: TPCSPVDTime")
    print(json.dumps(jdl_reco_tpcspvdtime, indent=2))
  if jdl_reco_cpass1:
    print("\n# Reco CPass1")
    print(json.dumps(jdl_reco_cpass1, indent=2))
  if jdl_reco_mergemakeocdbcpass1:
    print("\n# Reco CPass1: merge and make OCDB")
    print(json.dumps(jdl_reco_mergemakeocdbcpass1, indent=2))
  if jdl_fqa:
    print("\n# Final QA")
    print(json.dumps(jdl_fqa, indent=2))
    print("\n# QA Plots")
    print(json.dumps(jdl_qap, indent=2))
  exit(0)

# Summary
print("""Running the workflow with the following configuration:

Packages:
%(packages)s

%(numjobs)d total jobs, with job IDs from %(joba)d to %(jobb)d (included), will execute the command:
%(command)s

Required files (must be in the current directory, will be made available to each job):
%(reqd)s

Input files:
%(input)s

Output files (archives with content listed):
%(output)s

Output directory:
%(outdir)s

Environment variables available to the jobs:
%(env)s
""" % { "packages" : " * "+"\n * ".join(jdl["Packages"]) if not "EnvironmentCommand" in jdl else \
                     " * Custom environment command: " + jdl["EnvironmentCommand"],
        "numjobs"  : jdl["JobRange"][1]-jdl["JobRange"][0]+1,
        "joba"     : jdl["JobRange"][0],
        "jobb"     : jdl["JobRange"][1],
        "command"  : jdl["Executable"] + " " + jdl["SplitArguments"],
        "reqd"     : " * "+"\n * ".join(jdl["InputFile"]),
        "input"    : " * "+"\n * ".join(jdl["InputFilesFromList"]),
        "output"   : " * "+"\n * ".join([ " ==> ".join(x.split(":", 1)) for x in jdl["Output"] ]),
        "outdir"   : jdl["OutputDir"],
        "env"      : " * "+"\n * ".join([ "%s ==> %s"%(x,jdl["Environment"][x]) for x in jdl["Environment"]])

})
sys.stdout.flush()
if args.summary:
  exit(0)

# Create working directory. From now on we write to disk, not before
if args.force:
  try:
    rmtree(args.work_dir)
  except:
    pass
try:
  os.mkdir(args.work_dir)
except OSError as e:
  if e.errno == errno.EEXIST:
    print("Cannot create output directory \"%s\": remove existing one first" % args.work_dir)
  else:
    print("Cannot create output directory \"%s\": %s" % (args.workdir, e))
  exit(1)

# Create XML files and scripts for tiers > first
if jdl_reco_mergemakeocdbcpass0:
  with open(os.path.join(args.work_dir, "calibobjects.xml"), "w") as f: f.write(xml_reco)
  gen_runjob(os.path.join(args.work_dir, "runmergemakeocdbcpass0.sh"), jdl_reco_mergemakeocdbcpass0, args.dry_run)
if jdl_mft:
  with open(os.path.join(args.work_dir, "residualtrees.xml"), "w") as f: f.write(xml_mft)
  gen_runjob(os.path.join(args.work_dir, "runresidualmerge.sh"), jdl_mft, args.dry_run)
if jdl_reco_tpcspvdtime:
  with open(os.path.join(args.work_dir, "tpcSPCalibration.xml"), "w") as f: f.write(xml_tpcspcalibration)
  gen_runjob(os.path.join(args.work_dir, "runtpcspvdtime.sh"), jdl_reco_tpcspvdtime, args.dry_run)
if jdl_reco_cpass1:
  gen_runjob(os.path.join(args.work_dir, "runcpass1.sh"), jdl_reco_cpass1, args.dry_run)
if jdl_reco_mergemakeocdbcpass1:
  with open(os.path.join(args.work_dir, "calibobjects_cpass1.xml"), "w") as f: f.write(xml_calib_cpass1)
  gen_runjob(os.path.join(args.work_dir, "runmergemakeocdbcpass1.sh"), jdl_reco_mergemakeocdbcpass1, args.dry_run)
if jdl_fqa:
  with open(os.path.join(args.work_dir, inputfile_fqa), "w") as f: f.write(inputlist_fqa)
  gen_runjob(os.path.join(args.work_dir, "runfinalqa.sh"), jdl_fqa, args.dry_run)
  gen_runjob(os.path.join(args.work_dir, "runqaplots.sh"), jdl_qap, args.dry_run)

# Copy input files in the work directory
for f in [ i for k in all_inputs for i in all_inputs[k] ]:
  dest = os.path.join(args.work_dir, f)
  if os.path.isfile(dest) or re.search("^(/|[^ :]+:)", f): continue
  try:
    copy2(f, dest)  # current dir first
  except:
    try:
      copy2(os.path.join(lib_path, f), dest)  # fallback on installation path
    except IOError:
      print("Cannot copy input file \"%s\", please make it available in the current directory"
            " or remove it from the JDL" % f)
      exit(1)

# Remove from the list files that should not be copied by Makeflow
for k in dict(all_inputs):
  all_inputs[k] = [ i for i in all_inputs[k] if not re.search("^(/|[^ :]+:)", i) ]

# Produce the wrapper script for the jobs
gen_runjob(os.path.join(args.work_dir, "runjob.sh"), jdl, args.dry_run)

# Produce the Makeflow
makeflow = j2env.from_string("""# Automatically generated

{% for jobindex in range(joba,jobb) -%}
job{{ "%04d"|format(jobindex) }}.done: runjob.sh {{input_list["Jobs"]|join(" ")}}
	./runjob.sh {{jobindex}} job{{"%04d"|format(jobindex)}}.done{% if input_files|length > 1 %} {{input_files[jobindex-joba]}}{% endif %}
{% endfor -%}

{% if "MergeMakeOCDB_CPass0" in next_stages -%}
mergemakeocdbcpass0.done: runmergemakeocdbcpass0.sh {{input_list["MergeMakeOCDB_CPass0"]|join(" ")}}{% for jobindex in range(joba,jobb) %} job{{"%04d"|format(jobindex)}}.done{% endfor %}
	./runmergemakeocdbcpass0.sh 0 mergemakeocdbcpass0.done
{% endif -%}

{% if "MergeFilteredTrees" in next_stages -%}
mergefilteredtrees.done: runresidualmerge.sh {{input_list["MergeFilteredTrees"]|join(" ")}}{% for jobindex in range(joba,jobb) %} job{{"%04d"|format(jobindex)}}.done{% endfor %}
	./runresidualmerge.sh 0 mergefilteredtrees.done
{% endif -%}

{% if "TPCSPVDTime" in next_stages -%}
tpcspvdtime.done: runtpcspvdtime.sh {{input_list["TPCSPVDTime"]|join(" ")}} mergefilteredtrees.done mergemakeocdbcpass0.done
	./runtpcspvdtime.sh 0 tpcspvdtime.done
{% endif -%}

{% if "CPass1" in next_stages -%}
{% for jobindex in range(joba,jobb) -%}
cpass1reco{{ "%04d"|format(jobindex) }}.done: runcpass1.sh {{input_list["CPass1"]|join(" ")}} tpcspvdtime.done
	./runcpass1.sh {{jobindex}} cpass1reco{{"%04d"|format(jobindex)}}.done{% if input_files|length > 1 %} {{input_files[jobindex-joba]}}{% endif %}
{% endfor -%}
{% endif -%}

{% if "MergeMakeOCDB_CPass1" in next_stages -%}
mergemakeocdbcpass1.done: runmergemakeocdbcpass1.sh {{input_list["MergeMakeOCDB_CPass1"]|join(" ")}}{% for jobindex in range(joba,jobb) %} cpass1reco{{"%04d"|format(jobindex)}}.done{% endfor %}
	./runmergemakeocdbcpass1.sh 0 mergemakeocdbcpass1.done
{% endif -%}

{% if "FinalQA" in next_stages -%}
finalqa.done: runfinalqa.sh {{input_list["FinalQA"]|join(" ")}}{% for jobindex in range(joba,jobb) %} {% if "CPass1" in next_stages -%}cpass1reco{% else %}job{% endif %}{{"%04d"|format(jobindex)}}.done{% endfor %}
	./runfinalqa.sh 0 finalqa.done
qaplots.done: runqaplots.sh finalqa.done {{input_list["QAPlots"]|join(" ")}}
	./runqaplots.sh 0 qaplots.done
{% endif %}
""")
makeflow_fn = os.path.join(args.work_dir, "Makeflow")
with open(makeflow_fn, "w") as mf:
  mf.write(makeflow.render(pars        = jdl["SplitArguments"],
                           input_list  = all_inputs,
                           joba        = jdl["JobRange"][0],
                           jobb        = jdl["JobRange"][1]+1,
                           input_files = jdl["InputFilesFromList"],
                           next_stages = jdl.get("NextStages", [])))
if args.graph:
  wd = os.getcwd()
  os.chdir(args.work_dir)
  try:
    subprocess.check_call("makeflow_viz | dot -T pdf -o Makeflow.pdf", shell=True)
  except:
    print("ERROR: cannot generate workflow graph")
    exit(1)
  os.chdir(wd)

if not args.run:
  print("""Execute the workflow with:
      cd %(workdir)s
      makeflow %(makeflow_opts)s
  """ % { "workdir": args.work_dir, "makeflow_opts": " ".join(args.makeflow_opts) })

# Touch placeholder files to skip earlier stages
if args.start_at in [ "merge", "qaplots", "cpass0merge", "tpcspc", "cpass1", "cpass1merge", "cpass1qa", "cpass1qaplots" ]:
  for jobindex in range(jdl["JobRange"][0], jdl["JobRange"][1]+1):
    open(os.path.join(args.work_dir, "job%04d.done"%jobindex), "w").close()
if args.start_at in [ "tpcspc", "cpass1", "cpass1merge", "cpass1qa", "cpass1qaplots" ]:
  for done in [ "mergemakeocdbcpass0.done", "mergefilteredtrees.done" ]:
    open(os.path.join(args.work_dir, done), "w").close()
if args.start_at in [ "cpass1", "cpass1merge", "cpass1qa", "cpass1qaplots" ]:
  open(os.path.join(args.work_dir, "tpcspvdtime.done"), "w").close()
if args.start_at in [ "cpass1merge", "cpass1qa", "cpass1qaplots" ]:
  for jobindex in range(jdl_reco_cpass1["JobRange"][0], jdl_reco_cpass1["JobRange"][1]+1):
    open(os.path.join(args.work_dir, "cpass1reco%04d.done"%jobindex), "w").close()
if args.start_at == "qaplots":
  for done in [ "finalqa.done", "mergefilteredtrees.done" ]:
    open(os.path.join(args.work_dir, done), "w").close()
if args.start_at in [ "cpass1qa", "cpass1qaplots" ]:
  open(os.path.join(args.work_dir, "mergemakeocdbcpass1.done"), "w").close()
if args.start_at == "cpass1qaplots":
  open(os.path.join(args.work_dir, "finalqa.done"), "w").close()

# Run Makeflow in dryrun mode to pretend we have completed the earlier stages
if args.start_at != "sim" and args.start_at != "cpass0":
  wd = os.getcwd()
  os.chdir(args.work_dir)
  devnull = open(os.devnull)
  subprocess.check_call([ "makeflow", "-T", "dryrun" ], stdout=devnull, stderr=devnull)
  os.chdir(wd)

if args.run:
  os.chdir(args.work_dir)
  try:
    subprocess.check_call([ "time", "makeflow" ] + args.makeflow_opts)
  except subprocess.CalledProcessError as e:
    exit(e.returncode if e.returncode > 0 else 128-e.returncode)
  exit(0)
